\chapter*{‌پیوست}
\markboth{پیوست}{}
\addcontentsline{toc}{chapter}{پیوست}

\begin{latin}
\section*{\lr{English Summary}}
\lr{Nowadays, machine learning models are widely used in varied applications and issues. One of the important challenges for these models, especially deep ones, is adversarial learning and adversarial examples which are generated especially for machine learning models. Natural Language Processing is also an area in which adversarial learning and adversarial examples should be considered as a major concern. In this project, our purpose is to devise a defense system against text adversarial examples generated to fool NLP models. Masked language modeling, one of the phases in pretraining of deep BERT, is used to design an approach to identify adversarial examples in text. We also use pre-trained word embeddings (GLOVE) to find the correct form of those words identified as adversarial. To evaluate our method, the performance of our defense system is measured on three transformer-based models and three english text classification datasets. The results show that our method outperforms semi-character neural networks, another novel defensive method against text adversarial examples, in all experiments.}
\end{latin}